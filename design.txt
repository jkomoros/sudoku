dokugen must be able to solve Sudoku itself to be able to generate them.

A simple BFS of statespace on constraints should be fastest. But to get difficulty, we'll need to solve as a human does.

We'll have a fastSolve, which uses a constrained BFS, and a humanSolve, which proceeds knowing a grid can be solved and returns a difficulty.

Grids are passed back and forth over channels. They should be easily serializable and small whenever possible, but still be fast for common operations. You may read and write to them only if you have a ref, and you can only get a ref via a channel.


Grid {
	data string
	cells Cell[DIM * DIM]
}

TODO:

* Consider factoring out Rand methods to use a passthrough so we can test reasonably by setting a seed.

* Investigate why using the finite queue isn't making a big dent in Fill() performance.
* Reduce memory usage in cell: row, col, block are rarely asked for so can be derived?

* There's a leak somewhere in solve or fill. If you fill 500000 grids it runs out of memory.

* Some way to quickly copy queues and then enforce that all gets come from a copy; this will be useful for solver techniques which require a clean queue.
	* Queues have Getters that each consume the underlying data individually. Each queue has an insertionCounter variable that is updated whenever an insert happens. Each getter remembers the insertNumber it is keyed to; if it ever changes it throw out its work and starts again (keeping its ignoreMap the same). Each getter has an ignoreMap that it uses to ignore items it has already seen. Each time it switches to a new bucket it makes a copy of the bucket that has no holes and no nils an is randomized. It then hands off items from that bucket.

* Solve performance ideas
	* At each level, with some randomness (ideally weighted), diverge from DFS by changing which one we dive into and spin off the rest of the work items.

* Grid should suppot copy operations and have copy-on-write semantics with shared cells.
	* This is hard.
	* Grids can have parents. If they do, they are a "sparse grid".
		* No, should be a SparseGrid with a *Grid member.
		* SparseGrid has a Cell() method that looks in its cache. if it doesn't exist, looks to parent for one.
	* SparseCell is soemthing that looks like a cell but actually is a wrapper around one with a pointer back to the sparseGrid it's from.
		* Sparsecells have their own trapping of any modifiying calls that then call back to the sparseGrid (they know which it is) with the new cell itself to put in the cache. Then the SparseCell can be thrown away.
	* Simplest (no caching): Whenever you want a cell, you call grid.Cell(), which sees if it has one, otherwise returns one from another cell.

* Run dokugen/main.go and then panic at the end. Verify there aren't a billion goroutines hanging out.

* Make a decision about how to represent cell coordinates and row/col/block indexes when presented to user. now we're consistently 0 indexed (mostly), but shouldn't we be 1 indexed when presenting to the user?

* Human solve: Implement more solve tehcniques described at http://www.sadmansoftware.com/sudoku/solvingtechniques.htm. Technqiues I've skipped so far:
	* BlockBlock interactions
	* Should we consider a "guess between two options" to be a possible (extremely expensive) technique?
		* I don't think this actually works in the model I have (it could be possible to have to unwind.)
			* I think it's fine if it's a special technique that's not in the techniques array.
				* We'd want an AllTechniques array as well as NormalTechniques array. Analysis would use the former; solver would use the latter.
			* In Human Solve, if nothing else works, it Finds() with that Guess technique and randomly applies one.
			* But because Human Solve was the one that decided to apply it, and knows that it's special, it saves an "unwind to" point.
				* It would also have to know the "other" path to take at the unwind point.
			* The unwindTo points will need to be nested (although for a first stab we could just only allow one UnWind and bail otherwise)
			* As far as SolveDirections et al are concerned, Guess would just be a REALLY expensive technique.
	* Implement enough techniques to be able to solve #122, #33, and #138.

* Visualize the ranges of the different userSolveCollections in analysis, to verify that we're getting good coverage and overlap from them.

* Analysis: Spot check the results of the new markov difficulty grader to see if they intuitively make sense.
* Analysis: Run our difficulty grader on the real puzzles and see how well it fits with user perceived difficulty.
* Analysis: Throw out outliers for each individual user.
* Analysis: Throw out users who don't have a full range of difficulties in their history.
* Analysis: Seriously refactor the monster (and confusing!) manin method).
* Analysis: Don't include puzzles that have fewer than N solves (and/or that are disabled)
* Analysis: Allow the user to set a max number of multiplies from the command line

* A way for technique finders to import the results of other techniques. For example, onlylegalnumber should import obviousincollection results and not duplicate those. And hiddensubset and nakedsubset should trade their results as input to the others. It's not clear if this additional layers of calculation is worth it or not...

* Now that difficulties coming ouf of analysis are roughly linear from 0.1 to 0.9, clamp SolveDirections.Difficulty () to 0 and 1.0 (report when they would have exceeded) and put in the tuned weights

* With current weights, many generated puzzles are coming back with very low weights--they only need a small number of techniques to solve them. Why is that?

* Snap difficulties in SolveDirections.Difficulty, and alert of them there (we'll hear about them less)

* run the analysis on old versions and new checkouts and see if the sorted order of the puzzles changes much.

* Make it so NewGrid grabs a grid from grid cache if possible.
	* Actually call grid.Done() when we're done
	* We tried to do this but got CRAZY errors, which are hard to track down. Turns out that there is some possibility for slippage in grids, somehow. Need to solve that.
* When generating grids, at every step check the difficulty. If it's above the target, go back to the most recent known good difficulty and return that.

* When generating grids, keep a temporary directory of sdk files representing puzzles generated but not vended. When asked for a new puzzle, we first see if we have any in the unvended list that are appropriate difficulty.

* Graph the difficulties for a large number of generated puzzles and see if it follows any particular pattern.

NEXT STEP:
* Retrain difficulties with the fixed solver, they're clearly wrong.

* Does our use of the Rand convenience funcs in different go routines cause performance problems? (http://stackoverflow.com/questions/14298523/why-does-adding-concurrency-slow-down-this-golang-code)
	* In any case we want to be able to test randomness.

* In generator, make it so it's not STRICT about symmetry (the puzzles look too perfect now)

* Make it so the docs generated from dokugen are all useful.

* REdo of threaded solving
	* Why is performance so slow?

* Add an interactive mode to the command line tool.

* Fix races with go test -race.

* Test the main app (partially there... just need to minimize randomness now)

* If HumanSolve gets stuck, it should do a guess as a last resort.


* Make it so if any technique takes too long and we already have some answers, we move ahead anyway.

* Move all of the puzzle constants in tests into SDK files

* Use gofmt to replace all instances of t.Log(a) t.Fail() with t.Error.

* Running `go run main.go -s ../puzzles/test/harddifficulty.sdk -w` results in a panic.

* The difficulties we get when generating puzzles from app are absurdly low (even 0 sometimes!)



