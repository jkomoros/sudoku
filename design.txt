dokugen must be able to solve Sudoku itself to be able to generate them.

A simple BFS of statespace on constraints should be fastest. But to get difficulty, we'll need to solve as a human does.

We'll have a fastSolve, which uses a constrained BFS, and a humanSolve, which proceeds knowing a grid can be solved and returns a difficulty.

Grids are passed back and forth over channels. They should be easily serializable and small whenever possible, but still be fast for common operations. You may read and write to them only if you have a ref, and you can only get a ref via a channel.


Grid {
	data string
	cells Cell[DIM * DIM]
}

TODO:

* Consider factoring out Rand methods to use a passthrough so we can test reasonably by setting a seed.

* Investigate why using the finite queue isn't making a big dent in Fill() performance.
* Reduce memory usage in cell: row, col, block are rarely asked for so can be derived?

* There's a leak somewhere in solve or fill. If you fill 500000 grids it runs out of memory.

* Some way to quickly copy queues and then enforce that all gets come from a copy; this will be useful for solver techniques which require a clean queue.
	* Queues have Getters that each consume the underlying data individually. Each queue has an insertionCounter variable that is updated whenever an insert happens. Each getter remembers the insertNumber it is keyed to; if it ever changes it throw out its work and starts again (keeping its ignoreMap the same). Each getter has an ignoreMap that it uses to ignore items it has already seen. Each time it switches to a new bucket it makes a copy of the bucket that has no holes and no nils an is randomized. It then hands off items from that bucket.

* Solve performance ideas
	* At each level, with some randomness (ideally weighted), diverge from DFS by changing which one we dive into and spin off the rest of the work items.

* Grid should suppot copy operations and have copy-on-write semantics with shared cells.
	* This is hard.
	* Grids can have parents. If they do, they are a "sparse grid".
		* No, should be a SparseGrid with a *Grid member.
		* SparseGrid has a Cell() method that looks in its cache. if it doesn't exist, looks to parent for one.
	* SparseCell is soemthing that looks like a cell but actually is a wrapper around one with a pointer back to the sparseGrid it's from.
		* Sparsecells have their own trapping of any modifiying calls that then call back to the sparseGrid (they know which it is) with the new cell itself to put in the cache. Then the SparseCell can be thrown away.
	* Simplest (no caching): Whenever you want a cell, you call grid.Cell(), which sees if it has one, otherwise returns one from another cell.

* Rate sudokus based on experienced difficulty. (See http://www.longwood.edu/assets/mathematics/Team2975_ProblemB.pdf)
	* Also: http://www.sudokuwiki.org/Sudoku_Creation_and_Grading.pdf
	* Also: http://zhangroup.aporc.org/images/files/Paper_3485.pdf
	* Signals:
		* Number of steps
		* Steps * normalized difficulty rating of each step
		* Weight of hardest technique (or index cutoff of highest technique)
		* Avergae weight of difficulties
	* Should difficulties go up exponentionally? A technique that requires no candidate marking is WAY, WAY easier than one that requires multiple leaps of logic.
	* Actually calculating it:
		* What we have need is a Multiple Linear Regression.
		* Useful link: http://onlinestatbook.com/2/regression/multiple_regression.html
		* Further analysis: Linearize/normalize difficulties from phase 1?
		* Further analysis: load in difficulty weights into solver and run difficulty calcs again
		* Further analysis: Difficulty calc has a raw weight that it divides by
			* ... Won't this always be nessary, given that this is a sum of terms? Maybe we just clamp to 100 and 0.
		* Is it weird that we have some negative weights coming out of analysis? For example, won't that trip up the weightedrandomIndex?
			* Yes, it looks like negative weights will trip it up. If we find a negative weight we should run through and add -1 * the negative weight to all of them.
		* Another idea: I'm treating the userDifficulty as the actual difficulty, but really all it represents is the ordering. Try assigning a difficulty that is evenly spread across the range linearly (from maybe 0.1 to 0.9) and train on that.

* Run dokugen/main.go and then panic at the end. Verify there aren't a billion goroutines hanging out.

* Make a decision about how to represent cell coordinates and row/col/block indexes when presented to user. now we're consistently 0 indexed (mostly), but shouldn't we be 1 indexed when presenting to the user?

* Human solve: Is it worth it to do nakedSingles based on row/col/block as well? Those are obvious in different ways.
* Human solve: Implement more solve tehcniques described at http://www.sadmansoftware.com/sudoku/solvingtechniques.htm. Technqiues I've skipped so far:
	* BlockBlock interactions
	* Should we consider a "guess between two options" to be a possible (extremely expensive) technique?
		* I don't think this actually works in the model I have (it could be possible to have to unwind.)
			* I think it's fine if it's a special technique that's not in the techniques array.
				* We'd want an AllTechniques array as well as NormalTechniques array. Analysis would use the former; solver would use the latter.
			* In Human Solve, if nothing else works, it Finds() with that Guess technique and randomly applies one.
			* But because Human Solve was the one that decided to apply it, and knows that it's special, it saves an "unwind to" point.
			* The unwindTo points will need to be nested (although for a first stab we could just only allow one UnWind and bail otherwise)
			* As far as SolveDirections et al are concerned, Guess would just be a REALLY expensive technique.
	* Implement enough techniques to be able to solve #122, #33, and #138.

* Go Test is flaky. The description strings are somehow not matching for various techniques on a flaky basis on my MBP. Odd.

* Analysis: Spot check the results of the new markov difficulty grader to see if they intuitively make sense.
* Analysis: Run our difficulty grader on the real puzzles and see how well it fits with user perceived difficulty.
* Analysis: Throw out outliers for each individual user.
* Analysis: Throw out users who don't have a full range of difficulties in their history.
* Analysis: Seriously refactor the monster (and confusing!) manin method).
* Analysis: Don't include puzzles that have fewer than N solves (and/or that are disabled)
* Analysis: Allow the user to set a max number of multiplies from the command line

* Now that difficulties coming ouf of analysis are roughly linear from 0.1 to 0.9, clamp SolveDirections.Difficulty () to 0 and 1.0 (report when they would have exceeded) and put in the tuned weights

* Investigate making better grids (fewer clues / more symmetry)

* Make it so NewGrid grabs a grid from grid cache if possible.
	* Actually call grid.Done() when we're done
	* We tried to do this but got CRAZY errors, which are hard to track down. Turns out that there is some possibility for slippage in grids, somehow. Need to solve that.
* When generating grids, at every step check the difficulty. If it's above the target, go back to the most recent known good difficulty and return that.

* When generating grids, keep a temporary directory of sdk files representing puzzles generated but not vended. When asked for a new puzzle, we first see if we have any in the unvended list that are appropriate difficulty.

* Use machine learning to help us set the constants for difficulty correctly

* Does our use of the Rand convenience funcs in different go routines cause performance problems? (http://stackoverflow.com/questions/14298523/why-does-adding-concurrency-slow-down-this-golang-code)
	* In any case we want to be able to test randomness.

* Make it so the docs generated from dokugen are all useful.

* REdo of threaded solving
	* Why is performance so slow?

* Add an interactive mode to the command line tool.

* Fix races with go test -race.

* Test the main app (partially there... just need to minimize randomness now)

* If HumanSolve gets stuck, it should do a guess as a last resort.


* Make it so if any technique takes too long and we already have some answers, we move ahead anyway.

* Move all of the puzzle constants in tests into SDK files

* Use gofmt to replace all instances of t.Log(a) t.Fail() with t.Error.

* Running `go run main.go -s ../puzzles/test/harddifficulty.sdk -w` results in a panic.

* The difficulties we get when generating puzzles from app are absurdly low (even 0 sometimes!)



