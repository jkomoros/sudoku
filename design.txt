dokugen must be able to solve Sudoku itself to be able to generate them.

A simple BFS of statespace on constraints should be fastest. But to get difficulty, we'll need to solve as a human does.

We'll have a fastSolve, which uses a constrained BFS, and a humanSolve, which proceeds knowing a grid can be solved and returns a difficulty.

Grids are passed back and forth over channels. They should be easily serializable and small whenever possible, but still be fast for common operations. You may read and write to them only if you have a ref, and you can only get a ref via a channel.


Grid {
	data string
	cells Cell[DIM * DIM]
}

TODO:

* Consider factoring out Rand methods to use a passthrough so we can test reasonably by setting a seed.

* Investigate why using the finite queue isn't making a big dent in Fill() performance.
* Reduce memory usage in cell: row, col, block are rarely asked for so can be derived?

* There's a leak somewhere in solve or fill. If you fill 500000 grids it runs out of memory.

* Some way to quickly copy queues and then enforce that all gets come from a copy; this will be useful for solver techniques which require a clean queue.
	* Queues have Getters that each consume the underlying data individually. Each queue has an insertionCounter variable that is updated whenever an insert happens. Each getter remembers the insertNumber it is keyed to; if it ever changes it throw out its work and starts again (keeping its ignoreMap the same). Each getter has an ignoreMap that it uses to ignore items it has already seen. Each time it switches to a new bucket it makes a copy of the bucket that has no holes and no nils an is randomized. It then hands off items from that bucket.

* Solve performance ideas
	* At each level, with some randomness (ideally weighted), diverge from DFS by changing which one we dive into and spin off the rest of the work items.

* Grid should suppot copy operations and have copy-on-write semantics with shared cells.
	* This is hard.
	* Grids can have parents. If they do, they are a "sparse grid".
		* No, should be a SparseGrid with a *Grid member.
		* SparseGrid has a Cell() method that looks in its cache. if it doesn't exist, looks to parent for one.
	* SparseCell is soemthing that looks like a cell but actually is a wrapper around one with a pointer back to the sparseGrid it's from.
		* Sparsecells have their own trapping of any modifiying calls that then call back to the sparseGrid (they know which it is) with the new cell itself to put in the cache. Then the SparseCell can be thrown away.
	* Simplest (no caching): Whenever you want a cell, you call grid.Cell(), which sees if it has one, otherwise returns one from another cell.

* An alternate version of a sparseGrid that's must easier is a grid that is immutable, and when you make a new change it returns a new grid. Even this is hard; impossible counts will change quite a bit even if only one number is put in.

* Run dokugen/main.go and then panic at the end. Verify there aren't a billion goroutines hanging out.

* Make a decision about how to represent cell coordinates and row/col/block indexes when presented to user. now we're consistently 0 indexed (mostly), but shouldn't we be 1 indexed when presenting to the user?

* Human solve: Implement more solve tehcniques described at http://www.sadmansoftware.com/sudoku/solvingtechniques.htm. Technqiues I've skipped so far:
	* Implement enough techniques to be able to solve #122, #33, and #138.

* Visualize the ranges of the different userSolveCollections in analysis, to verify that we're getting good coverage and overlap from them.

* Analysis: Spot check the results of the new markov difficulty grader to see if they intuitively make sense.
* Analysis: Run our difficulty grader on the real puzzles and see how well it fits with user perceived difficulty.
* Analysis: Throw out outliers for each individual user.
* Analysis: Throw out users who don't have a full range of difficulties in their history.
* Analysis: Seriously refactor the monster (and confusing!) manin method).
* Analysis: Don't include puzzles that have fewer than N solves (and/or that are disabled)
* Analysis: Allow the user to set a max number of multiplies from the command line

* More signals to add:
		* How many cells at beginning have naked singles
		* Consider having a 1.0 or 0.0 for "has a step" so it's not how MANY, but whether it's in there at all.

* Consider just giving up and doing some python with scikit.py for learning the weights. Its documentation is much better...

* Verify that the regression treats Constant correctly. (I think it does now)

* Figure out why the regression coefficients look so fishy (similar ones for many of the techniques)

* A way for technique finders to import the results of other techniques. For example, onlylegalnumber should import obviousincollection results and not duplicate those. And hiddensubset and nakedsubset should trade their results as input to the others. It's not clear if this additional layers of calculation is worth it or not...

* Now that difficulties coming ouf of analysis are roughly linear from 0.1 to 0.9, clamp SolveDirections.Difficulty () to 0 and 1.0 (report when they would have exceeded) and put in the tuned weights

* With current weights, many generated puzzles are coming back with very low weights--they only need a small number of techniques to solve them. Why is that?

* Snap difficulties in SolveDirections.Difficulty, and alert of them there (we'll hear about them less)

* run the analysis on old versions and new checkouts and see if the sorted order of the puzzles changes much.

* Make it so NewGrid grabs a grid from grid cache if possible.
	* Actually call grid.Done() when we're done
	* We tried to do this but got CRAZY errors, which are hard to track down. Turns out that there is some possibility for slippage in grids, somehow. Need to solve that.
		* It's possible these come from some of the tests that dig deep into grids/cells and set things weird. Will have to audit.
			* hst_single_test looks like a good candidate.
* When generating grids, at every step check the difficulty. If it's above the target, go back to the most recent known good difficulty and return that.

* When generating grids, keep a temporary directory of sdk files representing puzzles generated but not vended. When asked for a new puzzle, we first see if we have any in the unvended list that are appropriate difficulty.

* Graph the difficulties for a large number of generated puzzles and see if it follows any particular pattern.

* Use golearn (or even scikit.py, if necessary) to train the difficulties, instead of relying on Weka.

* Does our use of the Rand convenience funcs in different go routines cause performance problems? (http://stackoverflow.com/questions/14298523/why-does-adding-concurrency-slow-down-this-golang-code)
	* In any case we want to be able to test randomness.
	* I tried it in Solve and was finding that generating a new r per thread was actually taking 110% of the time of just using the global one, oddly...

* Make it so the docs generated from dokugen are all useful.

* REdo of threaded solving
	* Why is performance so slow?

* Add an interactive mode to the command line tool.

* Grid.Difficulty can return NaN sometimes
	* When the grid isn't solveable with HumanSolve. ... But shouldn't that be impossible?
		* Maybe when human solve detects at the beginning that it has multiple solutions? But how would that even be generated by Grid?

* Dokugen should have a difficulty argument that can be Gentle, Easy, Intermediate, Hard, Impossible which sets the difficulties.

* In a perfect world, NonFill steps that don't end up being necessary to actually place a cell in the future would not be included.
	* This may be a side effect of other techinques taking a "key" cell (the cells affected last move) to focus more intensely on. This mirrors human behavior more closely.
		* The way to accomplish this is for Human Solve to sort a given cell's possibilities by how "close" they are to the last TargetCells, and tweak their probability
		to make the more quite a bit more likely.

* Picking an InverseWeightedIndex should probably use 1/x, not 1 -x.

* Grids should cache their difficulty, so they don't have to recalculate it

* GenerateGrid should take a maxDifficulty parameter, and as it's generating puzzles we stop removing cells as soon as it crosses that barrier.
	* This is hard since our difficulty weights are heavingly trained on realistic puzzles (with 50 filled cells or more)

* Our likelihood calculation should put blocks easier than rows and cols; yes there are scanning horizontal and vertical, but your eye doesn' thave to travel as far.

* Our current difficulty is only tuned for 'realistic' puzzles: puzzles that actually have a large number of cells filled.
	* We should include a lot of trivially easy puzzles in the difficulty training set (and really really hard ones, requiring tons of guesses) to make 
	sure we're training for a broad range of puzzles. We should inject these in (optionally) after phase 1.

* Write a sdk --> komorosk_sudoku converter

* Fix races with go test -race.

* Test the main app (partially there... just need to minimize randomness now)

* Start rolling out some of the generated puzzles into production to start accumulating solves on them

* Verify that userSolveRanges have a reasonable overlap

* Make it so if any technique takes too long and we already have some answers, we move ahead anyway.

* Move all of the puzzle constants in tests into SDK files

* Use gofmt to replace all instances of t.Log(a) t.Fail() with t.Error.

* Running `go run main.go -s ../puzzles/test/harddifficulty.sdk -w` says that the puzzle can't be solved... but should a guess happen?? (It has multiple solutions, but that shouldn't matter)

* The difficulties we get when generating puzzles from app are absurdly low (even 0 sometimes!)

== Generating Weights ==

cd to cmd/dokugen-analysis

go run main.go mock_data.go -a -v -p -w > relativedifficulties.csv
go run main.go mock_data.go -a -v -w -t -h relativedifficulties.csv > solves.csv

Load up Weka (the JVM version), load solves.csv, switch to Classify tab, select LeastMedSq (leave defaults -S 4), make sure (num) Difficulty is showing in the drop down.


