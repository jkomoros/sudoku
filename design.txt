dokugen must be able to solve Sudoku itself to be able to generate them.

A simple BFS of statespace on constraints should be fastest. But to get difficulty, we'll need to solve as a human does.

We'll have a fastSolve, which uses a constrained BFS, and a humanSolve, which proceeds knowing a grid can be solved and returns a difficulty.

Grids are passed back and forth over channels. They should be easily serializable and small whenever possible, but still be fast for common operations. You may read and write to them only if you have a ref, and you can only get a ref via a channel.


Grid {
	data string
	cells Cell[DIM * DIM]
}

TODO:

* Consider factoring out Rand methods to use a passthrough so we can test reasonably by setting a seed.

* Investigate why using the finite queue isn't making a big dent in Fill() performance.
* Reduce memory usage in cell: row, col, block are rarely asked for so can be derived?

* There's a leak somewhere in solve or fill. If you fill 500000 grids it runs out of memory.

* Some way to quickly copy queues and then enforce that all gets come from a copy; this will be useful for solver techniques which require a clean queue.
	* Queues have Getters that each consume the underlying data individually. Each queue has an insertionCounter variable that is updated whenever an insert happens. Each getter remembers the insertNumber it is keyed to; if it ever changes it throw out its work and starts again (keeping its ignoreMap the same). Each getter has an ignoreMap that it uses to ignore items it has already seen. Each time it switches to a new bucket it makes a copy of the bucket that has no holes and no nils an is randomized. It then hands off items from that bucket.

* Solve performance ideas
	* At each level, with some randomness (ideally weighted), diverge from DFS by changing which one we dive into and spin off the rest of the work items.

* Grid should suppot copy operations and have copy-on-write semantics with shared cells.
	* This is hard.
	* Grids can have parents. If they do, they are a "sparse grid".
		* No, should be a SparseGrid with a *Grid member.
		* SparseGrid has a Cell() method that looks in its cache. if it doesn't exist, looks to parent for one.
	* SparseCell is soemthing that looks like a cell but actually is a wrapper around one with a pointer back to the sparseGrid it's from.
		* Sparsecells have their own trapping of any modifiying calls that then call back to the sparseGrid (they know which it is) with the new cell itself to put in the cache. Then the SparseCell can be thrown away.
	* Simplest (no caching): Whenever you want a cell, you call grid.Cell(), which sees if it has one, otherwise returns one from another cell.

* An alternate version of a sparseGrid that's must easier is a grid that is immutable, and when you make a new change it returns a new grid. Even this is hard; impossible counts will change quite a bit even if only one number is put in.

* Run dokugen/main.go and then panic at the end. Verify there aren't a billion goroutines hanging out.

* Make a decision about how to represent cell coordinates and row/col/block indexes when presented to user. now we're consistently 0 indexed (mostly), but shouldn't we be 1 indexed when presenting to the user?

* Human solve: Implement more solve tehcniques described at http://www.sadmansoftware.com/sudoku/solvingtechniques.htm. Technqiues I've skipped so far:
	* Implement enough techniques to be able to solve #122, #33, and #138.

* Visualize the ranges of the different userSolveCollections in analysis, to verify that we're getting good coverage and overlap from them.

* Analysis: Spot check the results of the new markov difficulty grader to see if they intuitively make sense.
* Analysis: Run our difficulty grader on the real puzzles and see how well it fits with user perceived difficulty.
* Analysis: Throw out outliers for each individual user.
* Analysis: Throw out users who don't have a full range of difficulties in their history.
* Analysis: Seriously refactor the monster (and confusing!) manin method).
* Analysis: Don't include puzzles that have fewer than N solves (and/or that are disabled)
* Analysis: Allow the user to set a max number of multiplies from the command line

* Add more signals for difficulty (and train them)
	* Will need a generalized pipeline and lookup (and some way for the CSV of intermediate training to say which column is which)
	* More techniques that are for simple fill steps (that reflect how humans actually detect them). It's easier to reason through what these kinds of steps would be when playing puzzles by yourself
	* Make non-fill steps even double how difficulty they are right now. They're coming up way too often
	* Obvious-in-collection should be happening WAY more often than it is.
	* Feed into solve steps which number and where it was filled recently (to more accurately express chaining)
	* additional ideas:
		* Number of steps
		* number of non-fill steps (ideally compared to number of fill steps)
		* Number of steps before a non-fill is required
		* How many cells are filled at the beginning
		* How many cells at beginning have naked singles


* A way for technique finders to import the results of other techniques. For example, onlylegalnumber should import obviousincollection results and not duplicate those. And hiddensubset and nakedsubset should trade their results as input to the others. It's not clear if this additional layers of calculation is worth it or not...

* Now that difficulties coming ouf of analysis are roughly linear from 0.1 to 0.9, clamp SolveDirections.Difficulty () to 0 and 1.0 (report when they would have exceeded) and put in the tuned weights

* With current weights, many generated puzzles are coming back with very low weights--they only need a small number of techniques to solve them. Why is that?

* Snap difficulties in SolveDirections.Difficulty, and alert of them there (we'll hear about them less)

* run the analysis on old versions and new checkouts and see if the sorted order of the puzzles changes much.

* Make it so NewGrid grabs a grid from grid cache if possible.
	* Actually call grid.Done() when we're done
	* We tried to do this but got CRAZY errors, which are hard to track down. Turns out that there is some possibility for slippage in grids, somehow. Need to solve that.
		* It's possible these come from some of the tests that dig deep into grids/cells and set things weird. Will have to audit.
			* hst_single_test looks like a good candidate.
* When generating grids, at every step check the difficulty. If it's above the target, go back to the most recent known good difficulty and return that.

* When generating grids, keep a temporary directory of sdk files representing puzzles generated but not vended. When asked for a new puzzle, we first see if we have any in the unvended list that are appropriate difficulty.

* Graph the difficulties for a large number of generated puzzles and see if it follows any particular pattern.

NEXT STEP:
* Retrain difficulties with the fixed solver, they're clearly wrong.
	* Start out with everything having a constant weight and then run a few times from there.
		* Should that re-running from constant weight be baked in?


* Does our use of the Rand convenience funcs in different go routines cause performance problems? (http://stackoverflow.com/questions/14298523/why-does-adding-concurrency-slow-down-this-golang-code)
	* In any case we want to be able to test randomness.
	* I tried it in Solve and was finding that generating a new r per thread was actually taking 110% of the time of just using the global one, oddly...

* Make it so the docs generated from dokugen are all useful.

* REdo of threaded solving
	* Why is performance so slow?

* Add an interactive mode to the command line tool.

* Fix races with go test -race.

* Consider splitting out the liklihood of a user picking a technique from its difficulty. Right now it's weird that when you load the learned weights they are WAY different from the straightforwardly-generated ones. Also, anecdotally we don't appear to be as likely as we should be to pick easy techniques.

* Test the main app (partially there... just need to minimize randomness now)


* Make it so if any technique takes too long and we already have some answers, we move ahead anyway.

* Move all of the puzzle constants in tests into SDK files

* Use gofmt to replace all instances of t.Log(a) t.Fail() with t.Error.

* Running `go run main.go -s ../puzzles/test/harddifficulty.sdk -w` says that the puzzle can't be solved... but should a guess happen?? (It has multiple solutions, but that shouldn't matter)

* The difficulties we get when generating puzzles from app are absurdly low (even 0 sometimes!)



