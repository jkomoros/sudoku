{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Based on https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import csv\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 300\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "csv = numpy.genfromtxt(\"solves.csv\", skip_header=1, delimiter=\",\")\n",
    "train_difficulty = csv[:, 0]\n",
    "train_features = csv[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#number of training samples\n",
    "n_samples = train_difficulty.shape[0]\n",
    "\n",
    "#number of features\n",
    "feature_length = train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalize all of the columns to be between -0.5 and 0.5 with a smooth distribution\n",
    "#TODO: we need to save the maxVal so other folks in the model can normalize appropriately\n",
    "\n",
    "#TODO: figure out a better way to normalize these; right now many of them are squishing at -0.5 since their\n",
    "#distributions are often very left-skewed\n",
    "for colNum in xrange(0, feature_length):\n",
    "    col = train_features[:,colNum]\n",
    "    maxVal = max(col)\n",
    "    if maxVal == 0:\n",
    "        maxVal = 1\n",
    "    for index in xrange(0, len(col)):\n",
    "        col[index] = (col[index] - (maxVal / 2)) / maxVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# tf Graph Input\n",
    "Difficulty = tf.placeholder(\"float\")\n",
    "Features = tf.placeholder(\"float\", shape=(feature_length))\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.random_normal([feature_length], stddev=0.25), name=\"weight\")\n",
    "b = tf.Variable(0.0, name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a linear model\n",
    "pred = tf.add(tf.mul(Features, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: calculate R2 and output it over time\n",
    "# Mean squared error\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_sum(tf.pow(pred-Difficulty, 2))/(2*n_samples)\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "# Gradient descent\n",
    "with tf.name_scope(\"optimize\") as scope:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Summary ops to collect data\n",
    "#The use of tensorboard is adapted from the example on https://www.tensorflow.org/versions/r0.7/how_tos/summaries_and_tensorboard/index.html\n",
    "w_hist = tf.histogram_summary(\"weights\", W)\n",
    "b_hist = tf.histogram_summary(\"biases\", b)\n",
    "difficulty_hist = tf.histogram_summary(\"difficulty\", Difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.007192562 W= [-0.04834342 -0.13540412  0.35155329 -0.42981386  0.23225115  0.49575615\n",
      "  0.24197435  0.15733233  0.17822041  0.05280761  0.33208877  0.34243354\n",
      "  0.28202844  0.05939834 -0.30064267 -0.01703261 -0.18630184 -0.23021659\n",
      " -0.08854297  0.04416275  0.12727964 -0.25188053 -0.06973174  0.60302627\n",
      "  0.00253241 -0.13842016  0.3072648  -0.23878251  0.06400895 -0.57291526\n",
      " -0.16307452  0.03871728  0.20529599  0.08184347 -0.1058052  -0.43325245\n",
      "  0.43862256 -0.30968469  0.22350393 -0.23691283  0.04596709  0.35992762\n",
      " -0.05019029 -0.11165523 -0.05233024  0.05789621  0.06276163  0.04046629\n",
      " -0.36277628 -0.0810663  -0.14070025 -0.18129534 -0.01439438  0.00069876\n",
      "  0.18140323 -0.33203843 -0.152356   -0.17979912  0.15622851 -0.1982532\n",
      " -0.05431211  0.5803113   0.07454147 -0.25211942 -0.15584818  0.01258918\n",
      "  0.02979302 -0.06814001 -0.22866897 -0.08105121 -0.19036575  0.08265972\n",
      "  0.26668501 -0.12163173 -0.27324539  0.20305881  0.01028989 -0.52156901\n",
      "  0.03103366  0.00263892 -0.2288384  -0.17874892 -0.00290137  0.04076863\n",
      "  0.15954389  0.23915192] b= 0.538933\n",
      "Epoch: 0100 cost= 0.007137563 W= [-0.04196161 -0.11909616  0.35155329 -0.42981386  0.23225115  0.49575615\n",
      "  0.24197435  0.15733233  0.15804894  0.04703971  0.29646051  0.3057318\n",
      "  0.25322208  0.0550969  -0.26473245 -0.01197511 -0.16530991 -0.20412257\n",
      " -0.07913534  0.03799721  0.11155469 -0.22310093 -0.06973174  0.60302627\n",
      "  0.00141624 -0.12300885  0.27017355 -0.21174613  0.05542321 -0.5067057\n",
      " -0.14494197  0.03318449  0.18037556  0.07140272 -0.09138528 -0.38857305\n",
      "  0.39627153 -0.27561507  0.20310628 -0.20993286  0.04088951  0.31872004\n",
      " -0.04471925 -0.09904246 -0.04667855  0.05080953  0.05716516  0.03752103\n",
      " -0.32068577 -0.07097539 -0.12377565 -0.15957148 -0.0105118   0.00121077\n",
      "  0.18600692 -0.32622546 -0.1466188  -0.17965731  0.1487267  -0.18339527\n",
      " -0.05334996  0.54869664  0.07333132 -0.2418496  -0.15031767  0.00944601\n",
      "  0.03163855 -0.06762642 -0.20441826 -0.07121492 -0.16809921  0.07389995\n",
      "  0.23707604 -0.13117619 -0.24147971  0.17931631  0.0089888  -0.46119964\n",
      "  0.0280374   0.00305312 -0.20249775 -0.15820946 -0.00237829  0.03620264\n",
      "  0.14037198  0.21059904] b= 0.539751\n",
      "Epoch: 0150 cost= 0.007094239 W= [-0.03612157 -0.10445886  0.35155329 -0.42981386  0.23225115  0.49575615\n",
      "  0.24197435  0.15733233  0.1403861   0.04212268  0.26497996  0.2732898\n",
      "  0.22774385  0.05144616 -0.23255986 -0.00728766 -0.14659211 -0.18089832\n",
      " -0.07064048  0.03274752  0.09786602 -0.19750658 -0.06973174  0.60302627\n",
      "  0.00062284 -0.10921494  0.23762842 -0.18769312  0.04803804 -0.44808242\n",
      " -0.12874444  0.02849258  0.15857463  0.06237952 -0.07814413 -0.34779537\n",
      "  0.3584286  -0.24479657  0.18498163 -0.18548556  0.03658373  0.28243634\n",
      " -0.03970081 -0.08771812 -0.04149343  0.04473676  0.05237886  0.03508866\n",
      " -0.28319779 -0.06184625 -0.10858324 -0.14012575 -0.00678957  0.00163364\n",
      "  0.19055672 -0.32048884 -0.14096157 -0.17951323  0.14171758 -0.16925111\n",
      " -0.05246865  0.51872361  0.07207235 -0.23201506 -0.14496228  0.00655059\n",
      "  0.03343188 -0.06713562 -0.18300092 -0.06230769 -0.14817049  0.06632455\n",
      "  0.21101972 -0.14031613 -0.21322466  0.1585283   0.00802763 -0.4076477\n",
      "  0.02557188  0.00360678 -0.17899953 -0.13984559 -0.00172749  0.03234997\n",
      "  0.12363824  0.18558057] b= 0.540536\n",
      "Epoch: 0200 cost= 0.007060160 W= [ -3.07690427e-02  -9.13122222e-02   3.51553291e-01  -4.29813862e-01\n",
      "   2.32251152e-01   4.95756149e-01   2.41974354e-01   1.57332331e-01\n",
      "   1.24932133e-01   3.79510857e-02   2.37175748e-01   2.44628012e-01\n",
      "   2.05223277e-01   4.83670346e-02  -2.03724936e-01  -2.93784006e-03\n",
      "  -1.29886925e-01  -1.60213873e-01  -6.29586205e-02   2.82981265e-02\n",
      "   8.59664232e-02  -1.74731463e-01  -6.97317421e-02   6.03026271e-01\n",
      "   1.06286061e-04  -9.68566388e-02   2.09090456e-01  -1.66281730e-01\n",
      "   4.17042859e-02  -3.96153986e-01  -1.14260070e-01   2.45346054e-02\n",
      "   1.39518470e-01   5.45987040e-02  -6.59808591e-02  -3.10560882e-01\n",
      "   3.24629396e-01  -2.16901466e-01   1.68886647e-01  -1.63326263e-01\n",
      "   3.29530984e-02   2.50506341e-01  -3.50900777e-02  -7.75394142e-02\n",
      "  -3.67288180e-02   3.95520218e-02   4.83035780e-02   3.31032276e-02\n",
      "  -2.49792218e-01  -5.35781309e-02  -9.49366316e-02  -1.22710988e-01\n",
      "  -3.22114374e-03   1.97275821e-03   1.95056871e-01  -3.14818442e-01\n",
      "  -1.35390505e-01  -1.79367140e-01   1.35173395e-01  -1.55793071e-01\n",
      "  -5.16642593e-02   4.90216732e-01   7.07692429e-02  -2.22584352e-01\n",
      "  -1.39765710e-01   3.88779049e-03   3.51754613e-02  -6.66764006e-02\n",
      "  -1.64098993e-01  -5.42324148e-02  -1.30322993e-01   5.97888678e-02\n",
      "   1.88105538e-01  -1.49079800e-01  -1.88079387e-01   1.40343949e-01\n",
      "   7.35909212e-03  -3.60133588e-01   2.35687476e-02   4.27605817e-03\n",
      "  -1.58026546e-01  -1.23417154e-01  -9.71494475e-04   2.91201472e-02\n",
      "   1.09047890e-01   1.63672939e-01] b= 0.541287\n",
      "Epoch: 0250 cost= 0.007033376 W= [ -2.58558635e-02  -7.94929117e-02   3.51553291e-01  -4.29813862e-01\n",
      "   2.32251152e-01   4.95756149e-01   2.41974354e-01   1.57332331e-01\n",
      "   1.11427255e-01   3.44315656e-02   2.12629199e-01   2.19317809e-01\n",
      "   1.85328797e-01   4.57892455e-02  -1.77868068e-01   1.10389630e-03\n",
      "  -1.14966698e-01  -1.41780272e-01  -5.60023114e-02   2.45472398e-02\n",
      "   7.56385401e-02  -1.54452726e-01  -6.97317421e-02   6.03026271e-01\n",
      "  -1.73682478e-04  -8.57726112e-02   1.84079230e-01  -1.47207543e-01\n",
      "   3.62906046e-02  -3.50150198e-01  -1.01300098e-01   2.12168489e-02\n",
      "   1.22874826e-01   4.79068346e-02  -5.47987334e-02  -2.76554644e-01\n",
      "   2.94448882e-01  -1.91647381e-01   1.54608011e-01  -1.43230870e-01\n",
      "   2.99122874e-02   2.22420365e-01  -3.08468547e-02  -6.83795214e-02\n",
      "  -3.23439427e-02   3.51446234e-02   4.48532999e-02   3.15083601e-02\n",
      "  -2.20013753e-01  -4.60794009e-02  -8.26687366e-02  -1.07101887e-01\n",
      "   1.99899936e-04   2.23326264e-03   1.99498847e-01  -3.09202433e-01\n",
      "  -1.29897505e-01  -1.79220483e-01   1.29053086e-01  -1.42984509e-01\n",
      "  -5.09301536e-02   4.63197291e-01   6.94262460e-02  -2.13554025e-01\n",
      "  -1.34714767e-01   1.44312938e-03   3.68695296e-02  -6.62420839e-02\n",
      "  -1.47428617e-01  -4.69014347e-02  -1.14328310e-01   5.41689917e-02\n",
      "   1.67965233e-01  -1.57468095e-01  -1.65689945e-01   1.24450095e-01\n",
      "   6.94153411e-03  -3.17959130e-01   2.19671857e-02   5.04013291e-03\n",
      "  -1.39292061e-01  -1.08705245e-01  -1.30002401e-04   2.64339186e-02\n",
      "   9.63429883e-02   1.44504175e-01] b= 0.542007\n",
      "Epoch: 0300 cost= 0.007012275 W= [ -2.13388894e-02  -6.88556954e-02   3.51553291e-01  -4.29813862e-01\n",
      "   2.32251152e-01   4.95756149e-01   2.41974354e-01   1.57332331e-01\n",
      "   9.96401459e-02   3.14813070e-02   1.90971434e-01   1.96980804e-01\n",
      "   1.67767599e-01   4.36534584e-02  -1.54671982e-01   4.86424565e-03\n",
      "  -1.01628900e-01  -1.25338733e-01  -4.96935584e-02   2.14056652e-02\n",
      "   6.66905865e-02  -1.36384100e-01  -6.97317421e-02   6.03026271e-01\n",
      "  -2.52148748e-04  -7.58219212e-02   1.62175179e-01  -1.30204946e-01\n",
      "   3.16816680e-02  -3.09379876e-01  -8.96914229e-02   1.84569750e-02\n",
      "   1.08354107e-01   4.21690270e-02  -4.45144922e-02  -2.45498031e-01\n",
      "   2.67510086e-01  -1.68777436e-01   1.41953021e-01  -1.24997362e-01\n",
      "   2.73864772e-02   1.97730899e-01  -2.69351117e-02  -6.01257458e-02\n",
      "  -2.83008832e-02   3.14168185e-02   4.19505313e-02   3.02531701e-02\n",
      "  -1.93458006e-01  -3.92694771e-02  -7.16298595e-02  -9.31004882e-02\n",
      "   3.47973220e-03   2.41999002e-03   2.03890979e-01  -3.03650528e-01\n",
      "  -1.24485716e-01  -1.79079413e-01   1.23328634e-01  -1.30801558e-01\n",
      "  -5.02594449e-02   4.37548250e-01   6.80498853e-02  -2.04895452e-01\n",
      "  -1.29813552e-01  -7.96956476e-04   3.85182984e-02  -6.58259913e-02\n",
      "  -1.32741779e-01  -4.02375162e-02  -9.99812558e-02   4.93529588e-02\n",
      "   1.50279433e-01  -1.65508360e-01  -1.45742819e-01   1.10575028e-01\n",
      "   6.73853792e-03  -2.80517995e-01   2.07140520e-02   5.88101335e-03\n",
      "  -1.22547507e-01  -9.55210626e-02   7.80009781e-04   2.42219381e-02\n",
      "   8.52945000e-02   1.27747208e-01] b= 0.542698\n",
      "Optimization Finished!\n",
      "Training cost= 0.00701227 W= [ -2.13388894e-02  -6.88556954e-02   3.51553291e-01  -4.29813862e-01\n",
      "   2.32251152e-01   4.95756149e-01   2.41974354e-01   1.57332331e-01\n",
      "   9.96401459e-02   3.14813070e-02   1.90971434e-01   1.96980804e-01\n",
      "   1.67767599e-01   4.36534584e-02  -1.54671982e-01   4.86424565e-03\n",
      "  -1.01628900e-01  -1.25338733e-01  -4.96935584e-02   2.14056652e-02\n",
      "   6.66905865e-02  -1.36384100e-01  -6.97317421e-02   6.03026271e-01\n",
      "  -2.52148748e-04  -7.58219212e-02   1.62175179e-01  -1.30204946e-01\n",
      "   3.16816680e-02  -3.09379876e-01  -8.96914229e-02   1.84569750e-02\n",
      "   1.08354107e-01   4.21690270e-02  -4.45144922e-02  -2.45498031e-01\n",
      "   2.67510086e-01  -1.68777436e-01   1.41953021e-01  -1.24997362e-01\n",
      "   2.73864772e-02   1.97730899e-01  -2.69351117e-02  -6.01257458e-02\n",
      "  -2.83008832e-02   3.14168185e-02   4.19505313e-02   3.02531701e-02\n",
      "  -1.93458006e-01  -3.92694771e-02  -7.16298595e-02  -9.31004882e-02\n",
      "   3.47973220e-03   2.41999002e-03   2.03890979e-01  -3.03650528e-01\n",
      "  -1.24485716e-01  -1.79079413e-01   1.23328634e-01  -1.30801558e-01\n",
      "  -5.02594449e-02   4.37548250e-01   6.80498853e-02  -2.04895452e-01\n",
      "  -1.29813552e-01  -7.96956476e-04   3.85182984e-02  -6.58259913e-02\n",
      "  -1.32741779e-01  -4.02375162e-02  -9.99812558e-02   4.93529588e-02\n",
      "   1.50279433e-01  -1.65508360e-01  -1.45742819e-01   1.10575028e-01\n",
      "   6.73853792e-03  -2.80517995e-01   2.07140520e-02   5.88101335e-03\n",
      "  -1.22547507e-01  -9.55210626e-02   7.80009781e-04   2.42219381e-02\n",
      "   8.52945000e-02   1.27747208e-01] b= 0.542698 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged = tf.merge_all_summaries()\n",
    "    #TODO: figure out how to get these to overwrite existing logs (or at least be separate)\n",
    "    writer = tf.train.SummaryWriter(\"tmp/linear_logs\", sess.graph_def)\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (features, difficulty) in zip(train_features, train_difficulty):\n",
    "            sess.run(optimizer, feed_dict={Features: features, Difficulty: difficulty})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            result = sess.run([merged,cost], feed_dict={Features: features, Difficulty: difficulty})\n",
    "            summary_str = result[0]\n",
    "            c = result[1]\n",
    "            writer.add_summary(summary_str, epoch)\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "    training_cost = sess.run(cost, feed_dict={Features: features, Difficulty: difficulty})\n",
    "    print \"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: output the model with bias, cost, and weights all zipped up with their names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
