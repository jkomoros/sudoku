{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Based on https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import csv\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 300\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "csv = numpy.genfromtxt(\"solves.csv\", skip_header=1, delimiter=\",\")\n",
    "train_difficulty = csv[:, 0]\n",
    "train_features = csv[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#number of training samples\n",
    "n_samples = train_difficulty.shape[0]\n",
    "\n",
    "#number of features\n",
    "feature_length = train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalize all of the columns to be between 0 and 1 with a smooth distribution\n",
    "#TODO: we need to save the maxVal so other folks in the model can normalize appropriately\n",
    "\n",
    "for colNum in xrange(0, feature_length):\n",
    "    col = train_features[:,colNum]\n",
    "    maxVal = max(col)\n",
    "    if maxVal == 0:\n",
    "        maxVal = 1\n",
    "    for index in xrange(0, len(col)):\n",
    "        col[index] = col[index] / maxVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# tf Graph Input\n",
    "Difficulty = tf.placeholder(\"float\")\n",
    "Features = tf.placeholder(\"float\", shape=(feature_length))\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([feature_length]), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a linear model\n",
    "pred = tf.add(tf.mul(Features, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: calculate R2 and output it over time\n",
    "# Mean squared error\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_sum(tf.pow(pred-Difficulty, 2))/(2*n_samples)\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "# Gradient descent\n",
    "with tf.name_scope(\"optimize\") as scope:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Summary ops to collect data\n",
    "#The use of tensorboard is adapted from the example on https://www.tensorflow.org/versions/r0.7/how_tos/summaries_and_tensorboard/index.html\n",
    "w_hist = tf.histogram_summary(\"weights\", W)\n",
    "b_hist = tf.histogram_summary(\"biases\", b)\n",
    "difficulty_hist = tf.histogram_summary(\"difficulty\", Difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.006929512 W= [ 0.00215969  0.00216808  0.          0.          0.          0.          0.\n",
      "  0.          0.00158258  0.00156512  0.00257801  0.00263026  0.00332769\n",
      "  0.00351269  0.00450195  0.00459032  0.00033646  0.0003012   0.00021466\n",
      "  0.00022114  0.00041621  0.00044062  0.          0.          0.00039225\n",
      "  0.00037214  0.00019356  0.00019496  0.0001285   0.00012706  0.00020965\n",
      "  0.0002089   0.00040046  0.00037575  0.00590786  0.0067174   0.00385812\n",
      "  0.00430842  0.0038962   0.00455213  0.00146822  0.00144065  0.00080753\n",
      "  0.00075361  0.00079815  0.00087841  0.00272689  0.00288739  0.00213027\n",
      "  0.00216154  0.00225104  0.00245011  0.00369201  0.00095915  0.00651929\n",
      "  0.00350325  0.00449189 -0.0001453   0.0055797   0.0063279   0.00058004\n",
      " -0.00336011  0.00099547 -0.0024995   0.00294938 -0.00162742  0.00272066\n",
      "  0.00036354 -0.000378    0.00191732  0.00195985  0.00192128  0.00198255\n",
      " -0.01334671  0.00117908  0.00115726  0.00113866  0.00105449  0.00183845\n",
      "  0.00200087  0.00125343  0.00120451  0.00144716  0.001375    0.00075847\n",
      "  0.00066754] b= 0.544004\n",
      "Epoch: 0100 cost= 0.006929807 W= [  4.29190090e-03   4.30842070e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.14716320e-03   3.11236107e-03   5.12276357e-03   5.22593735e-03\n",
      "   6.60752552e-03   6.97240420e-03   8.94328859e-03   9.11757722e-03\n",
      "   6.68408757e-04   5.98226092e-04   4.28271363e-04   4.41183278e-04\n",
      "   8.29591008e-04   8.78190971e-04   0.00000000e+00   0.00000000e+00\n",
      "   7.82261137e-04   7.42187665e-04   3.86200147e-04   3.88976623e-04\n",
      "   2.55985273e-04   2.53112172e-04   4.17620235e-04   4.16124443e-04\n",
      "   7.98030174e-04   7.48824736e-04   1.16578126e-02   1.32265165e-02\n",
      "   7.64662866e-03   8.53080582e-03   7.72532681e-03   9.01524723e-03\n",
      "   2.91738985e-03   2.86270212e-03   1.60302781e-03   1.49627647e-03\n",
      "   1.58855063e-03   1.74780935e-03   5.41916257e-03   5.73633797e-03\n",
      "   4.23166296e-03   4.29337518e-03   4.47405223e-03   4.86762729e-03\n",
      "   5.90637932e-03   9.86117288e-04   1.15370173e-02   5.87256812e-03\n",
      "   7.64749665e-03  -8.63512745e-04   8.43605585e-03   1.02501819e-02\n",
      "   3.40347528e-04  -6.91452157e-03   1.04556198e-03  -5.18976478e-03\n",
      "   4.60801786e-03  -3.49807204e-03   4.38456330e-03   4.66650308e-05\n",
      "  -1.42771134e-03   3.81532428e-03   3.89970536e-03   3.82292527e-03\n",
      "   3.94442910e-03  -2.46176366e-02   2.34547281e-03   2.30216212e-03\n",
      "   2.26296298e-03   2.09596730e-03   3.65631841e-03   3.97779513e-03\n",
      "   2.49580992e-03   2.39846646e-03   2.87618442e-03   2.73314049e-03\n",
      "   1.50940090e-03   1.32861955e-03] b= 0.543898\n",
      "Epoch: 0150 cost= 0.006930223 W= [ 0.00641488  0.00643951  0.          0.          0.          0.          0.\n",
      "  0.          0.00470697  0.00465496  0.00765835  0.00781176  0.00987114\n",
      "  0.01041276  0.01335697  0.01361555  0.00099998  0.00089497  0.00064185\n",
      "  0.00066118  0.00124264  0.00131537  0.          0.          0.001172\n",
      "  0.00111201  0.0005788   0.00058296  0.00038346  0.00037916  0.00062554\n",
      "  0.0006233   0.00119524  0.00112162  0.0173435   0.01963759  0.01141726\n",
      "  0.01272742  0.01153734  0.01345013  0.00436169  0.00428015  0.00239576\n",
      "  0.0022367   0.00237785  0.00261565  0.00810031  0.00857188  0.00632377\n",
      "  0.00641545  0.00668798  0.00727327  0.0076836   0.00103332  0.01588009\n",
      "  0.0079175   0.01033297 -0.00147318  0.01033619  0.01317651  0.00017294\n",
      " -0.01038113  0.00111101 -0.00783009  0.00596075 -0.00531609  0.0058098\n",
      " -0.00020161 -0.00204381  0.00570891  0.00583492  0.00571977  0.00590105\n",
      " -0.03352747  0.00350836  0.00344376  0.00338373  0.0031346   0.00546807\n",
      "  0.00594661  0.00373638  0.00359077  0.0043008   0.00408747  0.00225869\n",
      "  0.00198857] b= 0.543804\n",
      "Epoch: 0200 cost= 0.006930673 W= [  8.52866378e-03   8.56127962e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.26195362e-03   6.19286951e-03   1.01847416e-02   1.03877094e-02\n",
      "   1.31184896e-02   1.38338627e-02   1.77430212e-02   1.80843025e-02\n",
      "   1.33116159e-03   1.19143073e-03   8.55376071e-04   8.81130807e-04\n",
      "   1.65535288e-03   1.75214768e-03   0.00000000e+00   0.00000000e+00\n",
      "   1.56144705e-03   1.48159801e-03   7.71369203e-04   7.76907487e-04\n",
      "   5.10921469e-04   5.05186676e-04   8.33414204e-04   8.30430712e-04\n",
      "   1.59207429e-03   1.49410777e-03   2.29647662e-02   2.59516090e-02\n",
      "   1.51698263e-02   1.68982148e-02   1.53319556e-02   1.78565755e-02\n",
      "   5.80105651e-03   5.69297234e-03   3.18569108e-03   2.97485385e-03\n",
      "   3.16602970e-03   3.48190358e-03   1.07702920e-02   1.13940267e-02\n",
      "   8.40654131e-03   8.52772687e-03   8.89277086e-03   9.66696907e-03\n",
      "   9.11202934e-03   1.09326083e-03   1.96394008e-02   9.68285184e-03\n",
      "   1.26188658e-02  -1.99008966e-03   1.16070323e-02   1.53635722e-02\n",
      "   6.06008216e-05  -1.37629416e-02   1.18531263e-03  -1.04219439e-02\n",
      "   7.06565566e-03  -7.08386069e-03   7.03113712e-03  -3.94050876e-04\n",
      "  -2.39566807e-03   7.59803969e-03   7.76539790e-03   7.61180837e-03\n",
      "   7.85230380e-03  -4.05692160e-02   4.66772495e-03   4.58200183e-03\n",
      "   4.50089341e-03   4.17035772e-03   7.27365445e-03   7.90733844e-03\n",
      "   4.97509073e-03   4.78144176e-03   5.72092971e-03   5.43800788e-03\n",
      "   3.00631975e-03   2.64736218e-03] b= 0.543722\n",
      "Epoch: 0250 cost= 0.006931200 W= [  1.06332349e-02   1.06736207e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.81207439e-03   7.72606954e-03   1.27017684e-02   1.29537117e-02\n",
      "   1.63494777e-02   1.72354989e-02   2.21015252e-02   2.25238372e-02\n",
      "   1.66193058e-03   1.48758083e-03   1.06885552e-03   1.10102445e-03\n",
      "   2.06771726e-03   2.18850281e-03   0.00000000e+00   0.00000000e+00\n",
      "   1.95060449e-03   1.85094192e-03   9.63891565e-04   9.70809662e-04\n",
      "   6.38363650e-04   6.31199102e-04   1.04122621e-03   1.03750173e-03\n",
      "   1.98852527e-03   1.86628930e-03   2.85222940e-02   3.21704112e-02\n",
      "   1.89052913e-02   2.10429430e-02   1.91089269e-02   2.22349223e-02\n",
      "   7.23548420e-03   7.10108271e-03   3.97279859e-03   3.71071720e-03\n",
      "   3.95305501e-03   4.34653182e-03   1.34290289e-02   1.42027158e-02\n",
      "   1.04799811e-02   1.06302211e-02   1.10884402e-02   1.20487325e-02\n",
      "   1.02621289e-02   1.16075028e-03   2.28939988e-02   1.12075061e-02\n",
      "   1.45656215e-02  -2.42750719e-03   1.24633024e-02   1.70018524e-02\n",
      "  -9.84029975e-06  -1.70623511e-02   1.26409507e-03  -1.29672149e-02\n",
      "   7.96982460e-03  -8.80329031e-03   8.07856210e-03  -5.40938112e-04\n",
      "  -2.58716289e-03   9.48259514e-03   9.69114620e-03   9.49884858e-03\n",
      "   9.79793537e-03  -4.61331196e-02   5.82353771e-03   5.71687520e-03\n",
      "   5.61442692e-03   5.20321541e-03   9.07303113e-03   9.85997543e-03\n",
      "   6.21183682e-03   5.97039331e-03   7.13659544e-03   6.78461557e-03\n",
      "   3.75226536e-03   3.30498302e-03] b= 0.543648\n",
      "Epoch: 0300 cost= 0.006931737 W= [  1.27285160e-02   1.27766170e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.35739186e-03   9.25456919e-03   1.52093563e-02   1.55096333e-02\n",
      "   1.95641816e-02   2.06179470e-02   2.64325049e-02   2.69342717e-02\n",
      "   1.99227897e-03   1.78341800e-03   1.28228322e-03   1.32086128e-03\n",
      "   2.47971877e-03   2.62442930e-03   0.00000000e+00   0.00000000e+00\n",
      "   2.33946182e-03   2.22003576e-03   1.15636643e-03   1.16466393e-03\n",
      "   7.65783887e-04   7.57190108e-04   1.24897342e-03   1.24450680e-03\n",
      "   2.38458393e-03   2.23815558e-03   3.40160690e-02   3.82937677e-02\n",
      "   2.26222686e-02   2.51617227e-02   2.28685346e-02   2.65845302e-02\n",
      "   8.66494514e-03   8.50446150e-03   4.75706439e-03   4.44425829e-03\n",
      "   4.73891990e-03   5.20950230e-03   1.60764847e-02   1.69980247e-02\n",
      "   1.25440629e-02   1.27228918e-02   1.32749267e-02   1.44186262e-02\n",
      "   1.11898147e-02   1.23204861e-03   2.57120915e-02   1.25248181e-02\n",
      "   1.62243694e-02  -2.79687322e-03   1.30458809e-02   1.82323437e-02\n",
      "  -4.86532954e-05  -2.02828012e-02   1.34425028e-03  -1.54661685e-02\n",
      "   8.71138182e-03  -1.04764588e-02   8.97746719e-03  -6.50783069e-04\n",
      "  -2.68212543e-03   1.13625228e-02   1.16120577e-02   1.13808839e-02\n",
      "   1.17382146e-02  -5.05251661e-02   6.97576348e-03   6.84836879e-03\n",
      "   6.72432920e-03   6.23312593e-03   1.08661437e-02   1.18044615e-02\n",
      "   7.44667137e-03   7.15762191e-03   8.54772795e-03   8.12732056e-03\n",
      "   4.49652085e-03   3.96141456e-03] b= 0.543581\n",
      "Optimization Finished!\n",
      "Training cost= 0.00693174 W= [  1.27285160e-02   1.27766170e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.35739186e-03   9.25456919e-03   1.52093563e-02   1.55096333e-02\n",
      "   1.95641816e-02   2.06179470e-02   2.64325049e-02   2.69342717e-02\n",
      "   1.99227897e-03   1.78341800e-03   1.28228322e-03   1.32086128e-03\n",
      "   2.47971877e-03   2.62442930e-03   0.00000000e+00   0.00000000e+00\n",
      "   2.33946182e-03   2.22003576e-03   1.15636643e-03   1.16466393e-03\n",
      "   7.65783887e-04   7.57190108e-04   1.24897342e-03   1.24450680e-03\n",
      "   2.38458393e-03   2.23815558e-03   3.40160690e-02   3.82937677e-02\n",
      "   2.26222686e-02   2.51617227e-02   2.28685346e-02   2.65845302e-02\n",
      "   8.66494514e-03   8.50446150e-03   4.75706439e-03   4.44425829e-03\n",
      "   4.73891990e-03   5.20950230e-03   1.60764847e-02   1.69980247e-02\n",
      "   1.25440629e-02   1.27228918e-02   1.32749267e-02   1.44186262e-02\n",
      "   1.11898147e-02   1.23204861e-03   2.57120915e-02   1.25248181e-02\n",
      "   1.62243694e-02  -2.79687322e-03   1.30458809e-02   1.82323437e-02\n",
      "  -4.86532954e-05  -2.02828012e-02   1.34425028e-03  -1.54661685e-02\n",
      "   8.71138182e-03  -1.04764588e-02   8.97746719e-03  -6.50783069e-04\n",
      "  -2.68212543e-03   1.13625228e-02   1.16120577e-02   1.13808839e-02\n",
      "   1.17382146e-02  -5.05251661e-02   6.97576348e-03   6.84836879e-03\n",
      "   6.72432920e-03   6.23312593e-03   1.08661437e-02   1.18044615e-02\n",
      "   7.44667137e-03   7.15762191e-03   8.54772795e-03   8.12732056e-03\n",
      "   4.49652085e-03   3.96141456e-03] b= 0.543581 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(\"tmp/linear_logs\", sess.graph_def)\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (features, difficulty) in zip(train_features, train_difficulty):\n",
    "            sess.run(optimizer, feed_dict={Features: features, Difficulty: difficulty})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            result = sess.run([merged,cost], feed_dict={Features: features, Difficulty: difficulty})\n",
    "            summary_str = result[0]\n",
    "            c = result[1]\n",
    "            writer.add_summary(summary_str, epoch)\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "    training_cost = sess.run(cost, feed_dict={Features: features, Difficulty: difficulty})\n",
    "    print \"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: output the model with bias, cost, and weights all zipped up with their names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
