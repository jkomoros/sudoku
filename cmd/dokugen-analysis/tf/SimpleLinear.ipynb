{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Based on https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import csv\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 300\n",
    "display_step = 50\n",
    "#We currently turn this off because it makes the model more difficult to apply to real data, and \n",
    "#because it doesn't appear to be necessary for convergence.\n",
    "DO_NORMALIZE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "csv = numpy.genfromtxt(\"solves.csv\", skip_header=1, delimiter=\",\")\n",
    "train_difficulty = csv[:, 0]\n",
    "train_features = csv[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#number of training samples\n",
    "n_samples = train_difficulty.shape[0]\n",
    "\n",
    "#number of features\n",
    "feature_length = train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalize all of the columns to be between -0.5 and 0.5 with a smooth distribution\n",
    "if DO_NORMALIZE:\n",
    "    #TODO: we need to save the maxVal so other folks in the model can normalize appropriately\n",
    "    #TODO: figure out a better way to normalize these; right now many of them are squishing at -0.5 since their\n",
    "    #distributions are often very left-skewed\n",
    "    print(\"Normalizing\")\n",
    "    for colNum in xrange(0, feature_length):\n",
    "        col = train_features[:,colNum]\n",
    "        maxVal = max(col)\n",
    "        if maxVal == 0:\n",
    "            maxVal = 1\n",
    "        for index in xrange(0, len(col)):\n",
    "            col[index] = (col[index] - (maxVal / 2)) / maxVal\n",
    "else:\n",
    "    print(\"Skipping normalizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# tf Graph Input\n",
    "Difficulty = tf.placeholder(\"float\")\n",
    "Features = tf.placeholder(\"float\", shape=(feature_length))\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.random_normal([feature_length], stddev=0.25), name=\"weight\")\n",
    "b = tf.Variable(0.0, name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a linear model\n",
    "pred = tf.add(tf.mul(Features, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: calculate R2 and output it over time\n",
    "# Mean squared error\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_sum(tf.pow(pred-Difficulty, 2))/(2*n_samples)\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "# Gradient descent\n",
    "with tf.name_scope(\"optimize\") as scope:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Summary ops to collect data\n",
    "#The use of tensorboard is adapted from the example on https://www.tensorflow.org/versions/r0.7/how_tos/summaries_and_tensorboard/index.html\n",
    "w_hist = tf.histogram_summary(\"weights\", W)\n",
    "b_hist = tf.histogram_summary(\"biases\", b)\n",
    "difficulty_hist = tf.histogram_summary(\"difficulty\", Difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.006754194 W= [-0.12339126 -0.46053571 -0.04670807 -0.19390152  0.21711847  0.2669456\n",
      "  0.02571134  0.1887003   0.1517283   0.02213729  0.00227103  0.20010909\n",
      " -0.06192914 -0.25108793  0.24930021  0.21471448 -0.27096069 -0.11343871\n",
      " -0.00409989 -0.294236    0.26401386  0.21977346 -0.11212447 -0.03615963\n",
      "  0.15228313  0.32542157  0.49856156  0.0852884  -0.4309364   0.08565165\n",
      "  0.0243285   0.2478058  -0.01933379 -0.09320458  0.15058093 -0.64584363\n",
      "  0.12525648  0.21167934 -0.21437828 -0.37254348  0.33566993 -0.40179864\n",
      " -0.13289675 -0.57010233  0.03365078 -0.61660826 -0.24922524  0.32886368\n",
      "  0.4356609  -0.4295609  -0.04322536 -0.24863817  0.0103194   0.18855175\n",
      "  0.0094152   0.26110479  0.00919067 -0.28044772  0.00787557  0.00747317\n",
      "  0.00506957 -0.29148462  0.00531697  0.19599319  0.00742508  0.3150008\n",
      "  0.00904437  0.22432071  0.39174289  0.1839904   0.10398999  0.34787416\n",
      "  0.07707904  0.00961297 -0.25711244 -0.35950601  0.0506169   0.38394645\n",
      "  0.16658008  0.33094358  0.21533211  0.13209169  0.39706078  0.05043106\n",
      " -0.0142271   0.45459065] b= 0.533744\n",
      "Epoch: 0100 cost= 0.006718992 W= [-0.11672259 -0.46046418 -0.04670807 -0.19390152  0.21711847  0.2669456\n",
      "  0.02571134  0.1887003   0.15282743  0.02216453  0.00923453  0.20022556\n",
      " -0.04903001 -0.2509141   0.24270679  0.21500511 -0.270183   -0.11343366\n",
      " -0.00387987 -0.29423302  0.26415375  0.21978076 -0.11212447 -0.03615963\n",
      "  0.15247072  0.32542455  0.49860775  0.08528952 -0.43075609  0.08565314\n",
      "  0.0245291   0.24780953 -0.01890262 -0.09319787  0.14693178 -0.64541447\n",
      "  0.1288327   0.21198927 -0.17019768 -0.37221646  0.33365178 -0.40174946\n",
      " -0.13154773 -0.57008743  0.03504261 -0.61658442 -0.23112674  0.32900077\n",
      "  0.43039322 -0.42947811 -0.03636087 -0.24854504  0.01018609  0.18388921\n",
      "  0.0092082   0.25934792  0.00901448 -0.27524942  0.00784261  0.00744229\n",
      "  0.00427385 -0.29171711  0.00461354  0.19464669  0.00647541  0.31398901\n",
      "  0.00888953  0.22013536  0.24391946  0.18192403  0.10414336  0.33880785\n",
      "  0.07721084  0.00956115 -0.25505176 -0.35948664  0.05167967  0.38396284\n",
      "  0.16800377  0.33098531  0.21635565  0.13212968  0.39452741  0.05048035\n",
      " -0.01341746  0.45460406] b= 0.535558\n",
      "Epoch: 0150 cost= 0.006705343 W= [-0.11020207 -0.46039358 -0.04670807 -0.19390152  0.21711847  0.2669456\n",
      "  0.02571134  0.1887003   0.15391412  0.02219172  0.01595633  0.20034117\n",
      " -0.03679735 -0.25074124  0.2366558   0.21529442 -0.26940912 -0.11342882\n",
      " -0.00366071 -0.29423004  0.26429105  0.21978746 -0.11212447 -0.03615963\n",
      "  0.1526572   0.32542753  0.49865311  0.08529063 -0.43057635  0.08565463\n",
      "  0.02472842  0.24781325 -0.01847376 -0.09319117  0.14371343 -0.64498532\n",
      "  0.13184607  0.21229747 -0.13138077 -0.37189206  0.33164164 -0.40170029\n",
      " -0.13020931 -0.57007253  0.03641588 -0.61656058 -0.21379292  0.32913646\n",
      "  0.4252187  -0.42939615 -0.02971576 -0.24845262  0.01010719  0.17919664\n",
      "  0.00908598  0.25750524  0.00891033 -0.27027556  0.00782317  0.00742409\n",
      "  0.00394496 -0.29198661  0.00429301  0.19324774  0.00613543  0.31293541\n",
      "  0.00879794  0.21588972  0.15194191  0.17993768  0.10429647  0.33014414\n",
      "  0.07734214  0.00953052 -0.25300455 -0.35946727  0.0527302   0.38397923\n",
      "  0.16940099  0.33102703  0.21736124  0.13216737  0.39201331  0.05052857\n",
      " -0.01261386  0.45461747] b= 0.536627\n",
      "Epoch: 0200 cost= 0.006700142 W= [-0.10381664 -0.46032354 -0.04670807 -0.19390152  0.21711847  0.2669456\n",
      "  0.02571134  0.1887003   0.15499206  0.02221868  0.02246038  0.2004559\n",
      " -0.0251753  -0.25056839  0.23112963  0.21558201 -0.26863751 -0.11342397\n",
      " -0.00344209 -0.29422706  0.26442677  0.21979417 -0.11212447 -0.03615963\n",
      "  0.15284304  0.32543051  0.49869782  0.08529175 -0.43039754  0.08565612\n",
      "  0.02492694  0.24781698 -0.0180465  -0.09318446  0.14092928 -0.64455795\n",
      "  0.13441747  0.2126039  -0.09723148 -0.37156934  0.32965124 -0.40165111\n",
      " -0.12887946 -0.57005763  0.03777438 -0.61653674 -0.19717436  0.32927087\n",
      "  0.42014638 -0.4293142  -0.02327136 -0.24836023  0.01006015  0.17454043\n",
      "  0.00901303  0.25561589  0.0088482  -0.26546025  0.00781167  0.00741333\n",
      "  0.00374829 -0.29228103  0.00410123  0.19182236  0.00593236  0.31185579\n",
      "  0.00874331  0.21165386  0.09472556  0.17804469  0.10444941  0.32188347\n",
      "  0.07747327  0.00951234 -0.25096944 -0.3594479   0.05377134  0.38399562\n",
      "  0.17077786  0.33106875  0.2183526   0.13220462  0.38952056  0.05057635\n",
      " -0.01181469  0.45463088] b= 0.537257\n",
      "Epoch: 0250 cost= 0.006698244 W= [-0.09755829 -0.46025351 -0.04670807 -0.19390152  0.21711847  0.2669456\n",
      "  0.02571134  0.1887003   0.15606259  0.0222456   0.02876314  0.20057064\n",
      " -0.01412023 -0.25039554  0.22609875  0.21586961 -0.26786771 -0.11341913\n",
      " -0.00322382 -0.29422408  0.26456237  0.21980087 -0.11212447 -0.03615963\n",
      "  0.15302856  0.32543349  0.49874252  0.08529287 -0.43021873  0.08565761\n",
      "  0.02512498  0.24782071 -0.01762041 -0.09317776  0.13855839 -0.64413178\n",
      "  0.13663709  0.21290977 -0.0671595  -0.37124747  0.32768825 -0.40160194\n",
      " -0.1275575  -0.57004273  0.03912024 -0.61651289 -0.18123205  0.32940349\n",
      "  0.4151803  -0.42923224 -0.0170145  -0.24826784  0.0100326   0.16995959\n",
      "  0.00897026  0.253712    0.00881179 -0.26076713  0.00780502  0.0074071\n",
      "  0.00363262 -0.29259351  0.00398843  0.19038109  0.00581287  0.3107585\n",
      "  0.00871128  0.20746124  0.05914274  0.17625095  0.10460177  0.31401578\n",
      "  0.07760407  0.00950176 -0.24894091 -0.35942852  0.05480546  0.38401201\n",
      "  0.17213711  0.33111048  0.21933082  0.13224187  0.38705626  0.05062403\n",
      " -0.01101893  0.45464429] b= 0.537621\n",
      "Epoch: 0300 cost= 0.006697650 W= [-0.09142108 -0.46018347 -0.04670807 -0.19390152  0.21711847  0.2669456\n",
      "  0.02571134  0.1887003   0.15712905  0.02227251  0.03487647  0.20068538\n",
      " -0.00359642 -0.25022268  0.22152936  0.21615703 -0.2671003  -0.11341429\n",
      " -0.00300581 -0.2942211   0.2646977   0.21980758 -0.11212447 -0.03615963\n",
      "  0.15321408  0.32543647  0.49878722  0.08529399 -0.43003991  0.0856591\n",
      "  0.02532261  0.24782443 -0.01719517 -0.09317105  0.13654949 -0.64370561\n",
      "  0.13856514  0.21321525 -0.04066755 -0.37092561  0.32574633 -0.40155277\n",
      " -0.1262417  -0.57002783  0.04045481 -0.61648905 -0.16593111  0.32953611\n",
      "  0.41032127 -0.42915028 -0.01093537 -0.24817546  0.01001696  0.1654702\n",
      "  0.00894596  0.25180793  0.00879111 -0.25618002  0.00780133  0.00740365\n",
      "  0.00356663 -0.29291186  0.00392408  0.18893166  0.00574466  0.30965462\n",
      "  0.0086931   0.20332643  0.03702296  0.17455788  0.10475412  0.30652636\n",
      "  0.07773483  0.00949582 -0.24692158 -0.35940915  0.05583331  0.3840284\n",
      "  0.17348291  0.3311522   0.22030042  0.13227913  0.38462108  0.05067172\n",
      " -0.01022601  0.4546577 ] b= 0.537821\n",
      "Optimization Finished!\n",
      "Training cost= 0.00669765 W= [-0.09142108 -0.46018347 -0.04670807 -0.19390152  0.21711847  0.2669456\n",
      "  0.02571134  0.1887003   0.15712905  0.02227251  0.03487647  0.20068538\n",
      " -0.00359642 -0.25022268  0.22152936  0.21615703 -0.2671003  -0.11341429\n",
      " -0.00300581 -0.2942211   0.2646977   0.21980758 -0.11212447 -0.03615963\n",
      "  0.15321408  0.32543647  0.49878722  0.08529399 -0.43003991  0.0856591\n",
      "  0.02532261  0.24782443 -0.01719517 -0.09317105  0.13654949 -0.64370561\n",
      "  0.13856514  0.21321525 -0.04066755 -0.37092561  0.32574633 -0.40155277\n",
      " -0.1262417  -0.57002783  0.04045481 -0.61648905 -0.16593111  0.32953611\n",
      "  0.41032127 -0.42915028 -0.01093537 -0.24817546  0.01001696  0.1654702\n",
      "  0.00894596  0.25180793  0.00879111 -0.25618002  0.00780133  0.00740365\n",
      "  0.00356663 -0.29291186  0.00392408  0.18893166  0.00574466  0.30965462\n",
      "  0.0086931   0.20332643  0.03702296  0.17455788  0.10475412  0.30652636\n",
      "  0.07773483  0.00949582 -0.24692158 -0.35940915  0.05583331  0.3840284\n",
      "  0.17348291  0.3311522   0.22030042  0.13227913  0.38462108  0.05067172\n",
      " -0.01022601  0.4546577 ] b= 0.537821 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "\n",
    "#TODO: do STOCHASTIC gradient descent\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged = tf.merge_all_summaries()\n",
    "    #TODO: figure out how to get these to overwrite existing logs (or at least be separate)\n",
    "    writer = tf.train.SummaryWriter(\"tmp/linear_logs\", sess.graph_def)\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (features, difficulty) in zip(train_features, train_difficulty):\n",
    "            sess.run(optimizer, feed_dict={Features: features, Difficulty: difficulty})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            result = sess.run([merged,cost], feed_dict={Features: features, Difficulty: difficulty})\n",
    "            summary_str = result[0]\n",
    "            c = result[1]\n",
    "            writer.add_summary(summary_str, epoch)\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "    training_cost = sess.run(cost, feed_dict={Features: features, Difficulty: difficulty})\n",
    "    print \"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: output the model with bias, cost, and weights all zipped up with their names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
